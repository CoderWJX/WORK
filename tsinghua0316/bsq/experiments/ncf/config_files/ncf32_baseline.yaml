# Experiment name
name: ncf32merge-baseline

output_dir: out

debug: true

# Device to be used
device:
  # Use CPU or GPU (choices: cpu, cuda)
  type: cpu
  # GPU device IDs to be used. Only valid when device.type is 'cuda'
  #gpu: [1,]

# dataset name
dataloader:
  dataset: 'ml-1m'
  #assert dataset in ['ml-1m', 'pinterest-20']
  path: '/home/duanhuiliu/tsinghua1215/datasets/ncf/ml-1m/'
  batch_size: 256
  top_k: 10
  num_ng: 4
  test_num_ng: 99
  batch_size: 256
  # Number of data loading workers
  workers: 8
  # Seeds random generators in a deterministic way (i.e., set all the seeds 0).
  # Please keep it true when resuming the experiment from a checkpoint
  deterministic: false
  user_num: 6040
  item_num: 3706
  

arch: ncf32


#'baseline','quant','inference'
task_name: 'baseline'

# pretrained_path: /root/code/quant/bsq-net/bsq/out/ncf32merge-baseline_20211207-232548/ncf32merge-baseline_best.pth

#GMF_path: /root/code/quant/bsq-net/bsq/out/gmf32_baseline_20211128-131645/gmf32_baseline_best.pth
GMF_path: /home/duanhuiliu/tsinghua1215/experiments/ncf/out/gmf32_baseline_20211215-134333/gmf32_baseline_best.pth
# MLP_path: /root/code/quant/NCF/NCF/out/MLP32_20211106-110955/MLP32_model.pth
#MLP_path: /root/code/quant/bsq-net/bsq/out/mlp32_baseline_20211210-123521/mlp32_baseline_best.pth
MLP_path: /home/duanhuiliu/tsinghua1215/experiments/ncf/out/mlp32_baseline_20211215-130407/mlp32_baseline_best.pth 

# GMF_path: /root/code/quant/NCF/NCF/out/GMF32_20211106-114617/GMF32_model.pth
# MLP_path: /root/code/quant/NCF/NCF/out/MLP32_20211106-010124/MLP32_model.pth
# #效果为：72.169 43.769




quan:
  ptq_batches: 1000
  act: # (default for all layers)
    # Quantizer type (choices: actquan)
    mode: actquan
    # Bit width of quantized activation
    bit: 8
    all_positive: true
    per_channel: false
    symmetric: false

  weight: # (default for all layers)
    # Quantizer type (choices: weightquan)
    mode: weightquan
    bit: 8
    all_positive: false
    per_channel: true
    symmetric: true
    t_gamma: 0.75
  excepts:
    # Specify quantized bit width for some layers, like this:
    predict_layer:
      act:
        bit: 
      weight:
        bit: 
    MLP_layers.1:
      act:
        bit: 8
        all_positive: false
        per_channel: false
        symmetric: true
      weight:
        bit: 8

epochs: 2

optimizer:
  type: SGD
  weight:
    learning_rate: 0.0001
    momentum: 0.9
    weight_decay: 0.000000000
  quant:
    learning_rate: 0.001
    momentum: 0.9
    weight_decay: 0.0
  other:
    learning_rate: 0.0001
    momentum: 0.9
    weight_decay: 0.000000000

# Learning rate scheduler
lr_scheduler:
  type: multistep
  milestones: [10, 16]
  gamma: 0.1
